# DeepLoco: Dynamic Locomotion Skills Using Hierarchical Deep Reinforcement Learning

A two level controller approach is used. Low level controller (LLC) operates at a fine timescale (i.e PD target angles) with the goal of limb control and balance, and a High level controller (HLC) that operates at a larger timescale (i.e walking steps) working to achieve long term goals such as best path through obstacles. This two-level hierarchy is used in 3D bipedal locomotion experiments with both levels using actor-critic policy gradient algorithms.

HLC has some long term goal and provides LLC with some low-level intermediate goal that directs joins to fullfil intermediate goal.

Heirarchal partitioning allows controllers to explore behvaiours across temporal and physical abstractions leading to more efficient exploration

HLC inputs are state (i.e robot config and environment), long term goal, outputs action (which translates into low-level intermediate goal). More specifically, action is a footstep plan for LLC. Updates to inputs are less frequent than updates to LLC.

LLC inputs are state (i.e mostly on robot config), low-level intermediate goal (provided by HLC), and outputs actions. More specifically, actions are specific PD target angles for actuators.

Environment provides rewards for both HLC and LLC (odd, since shouldn't HLC receive some sort of indication on the success of the LLC). 

Replay buffer is used to and minibatch GD is implemented. No mention of priority experience so I'm assuming it wasn't implemented.

Actor uses [CACLA][1] (actor-critic algorithm for continuous state and action spaces, updates only use sign of TD error as opposed to value as in A3C). Actions are represented as a Gaussian based on a mean parametrized by a neural network that is a function of state and goal. Gaussian noise is added during exploration. Policy gradient updates only occur if tuple was generated from taking an action with exploration noise. Epsilon greedy approaches for both HLC and LLC actors with epsilon annealing at different rates

Critic is trained to minimise MSE according to Bellman equations.

Reference motion is provided to LLC that consists of target poses for each timestep t. LLC state features include relative centers of mass to root (at pelvis), relative rotations and linear and angular velocities. Phase variable representing progress of current gait cycle is used to sync with reference motion. In implementation, phase variable was insufficient, so a sparse matrix was generated according to which range the phase variable is in (called bilinear phase transformation). 

LLC goal is a 2 step lookahead, i.e desired position for current leg in motion as well as desired position for next step, and root direction of character

Reference motion is preprocessed according to LLC goal parameters. During training, clips most similar to current long term goal (i.e minimizes L1 norm) is selected and the refernce controller associated with the clip is used to guide motion and shape LLC reward. More specifically LLC reward is a weighted sum that encourages imitation of reference motion while following footstep plan (LLC goal). 

Authors attribute robustness in design to exploration noise which perturbs motion and forces the learning of recovery actions

LLC network applies bilinear phase transform and outputs gaussian mean associated with current state and foot plan (LLC goal).

Training is on fixed time frame starting in a fixed posed. If character falls, remaining time frame results in 0 reward. New step position is generated by moving current position forward 0.4m in direction of root.

A style parameter can be used in the reward calculation to enforce a specific type of motion (i.e high knees, locked knees, more/less leaning, etc). LLC controllers with different styles can be linearly interpolated providing seamless results.

During HLC training, LLC weights are frozen. Action of HLC actor is footplan goal for LLC.

HLC network consists of some robot state measurements and terrain image which is preprocessed using CNN.

HLC reward in path planning is a function of the relative directions and distance between character and goal. It is designed to encourage character to move exponentiallyl towards goal.

LLC with specific styles can be swapped in for the nominal LLC that was used to train HLC with similar performance after fine tuning for various tasks. This demonstrates the transferability of the HLC and LLC controllers.

Since HLC output is LLC goal (footplan), HLC may exploit footplans for which the LLC had not been trained. Simply, HLC may demand unattainable footsteps from the LLC. During training, however, HLC will learn to avoid unattainable footplans as a result of failure from these outputs to LLC (i.e falling).

Paper: [DeepLoco: Dynamic Locomotion Skills Using Hierarchical Deep Reinforcement Learning](https://www.cs.ubc.ca/~van/papers/2017-TOG-deepLoco/2017-TOG-deepLoco.pdf)

[1]: https://hadovanhasselt.files.wordpress.com/2015/12/reinforcement_learning_in_continuous_action_spaces.pdf
